# Flink安装
* 新建docker-compose.yml文件，打开服务器的8081端口
```
version: "2.2"
services:
  jobmanager:
    image: flink:latest
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
      - SET_CONTAINER_TIMEZONE=true
      - CONTAINER_TIMEZONE=Asia/Shanghai
      - TZ=Asia/Shanghai
  taskmanager:
    image: flink:latest
    depends_on:
      - jobmanager
    command: taskmanager
    scale: 1
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: jobmanager
        taskmanager.numberOfTaskSlots: 2
      - SET_CONTAINER_TIMEZONE=true
      - CONTAINER_TIMEZONE=Asia/Shanghai
      - TZ=Asia/Shanghai
```
* 编译docker-compose.yml下载镜像
```
docker-compose up -d
```

## 清洗数据
> 流程设计：项目中通过`logUtils.print`进行数据埋点 > 将数据输出到`kafka` > `msg-stream`监听`MESSAGE_LOG`topic > `MsgSink`中数据分为用户和模板维度写入到redis中

* 项目部署：1.MsgFlinkConstant指定kafka和redis路径；2.`msg-stream`打成jar包(注意pom中要指定启动类路径)；3.提交到Flink后台中并运行

* 数据埋点
```
    logUtils.print(AnchorInfo.builder().state(AnchorState.SEND_SUCCESS.getCode()).businessId(taskInfo.getBusinessId()).ids(taskInfo.getReceiver()).build());
```

* 输出到kafka
```
public class LogUtils extends CustomLogListener {
        /**
         * 记录打点信息(将日志发送到kafka)
         */
        public void print(AnchorInfo anchorInfo) {
            anchorInfo.setTimestamp(System.currentTimeMillis());
            String message = JSON.toJSONString(anchorInfo);
            log.info(JSON.toJSONString(anchorInfo));
            try {
                kafkaUtils.send(topicName, message);
            } catch (Exception e) {
                log.error("LogUtils#print kafka fail! e:{},params:{}", Throwables.getStackTraceAsString(e)
                        , JSON.toJSONString(anchorInfo));
            }
        }
}
```

* 监听kafka的`MESSAGE_LOG`topic 
```
/**
 * flink启动类
 * @author wj
 * @create 2022-07-13 10:49
 */
@Slf4j
public class MsgBootStrap {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        /**
         * 1.获取KafkaConsumer
         */
        KafkaSource<String> kafkaConsumer = MessageQueueUtils.getKafkaConsumer(MsgFlinkConstant.TOPIC_NAME, MsgFlinkConstant.GROUP_ID, MsgFlinkConstant.BROKER);
        DataStreamSource<String> kafkaSource = env.fromSource(kafkaConsumer, WatermarkStrategy.noWatermarks(), MsgFlinkConstant.SOURCE_NAME);

        /**
         * 2. 数据转换处理
         */
        SingleOutputStreamOperator<AnchorInfo> dataStream = kafkaSource.flatMap(new MsgFlatMapFunction()).name(MsgFlinkConstant.FUNCTION_NAME);

        /**
         * 3. 将实时数据多维度写入Redis(已实现)，离线数据写入hive(未实现)
         */
        dataStream.addSink(new MsgSink()).name(MsgFlinkConstant.SINK_NAME);
        env.execute(MsgFlinkConstant.JOB_NAME);
    }
}
```

* 分维度写入到redis
```
/**
 * 消息进 redis/hive
 * @author wj
 * @create 2022-07-13 10:49
 */
@Slf4j
public class MsgSink  implements SinkFunction<AnchorInfo> {

    /**
     * batch的超时时间和大小
     */
    private static final Integer BATCH_SIZE = 10;
    private static final Long TIME_OUT = 2000L;

    /**
     * 用ThreadLocal来暂存数据做批量处理
     */
    private static ThreadLocal<List<AnchorInfo>> baseAnchors = ThreadLocal.withInitial(() -> new ArrayList<>(BATCH_SIZE));
    private static ThreadLocal<Long> lastClearTime = ThreadLocal.withInitial(() -> new Long(System.currentTimeMillis()));


    @Override
    public void invoke(AnchorInfo anchorInfo) throws Exception {
        realTimeData(anchorInfo);
        offlineDate(anchorInfo);
    }

    /**
     * 实时数据存入Redis
     * 1.用户维度(查看用户当天收到消息的链路详情)，数量级大，只保留当天
     * 2.消息模板维度(查看消息模板整体下发情况)，数量级小，保留30天
     *
     * @param anchorInfo
     */
    private void realTimeData(AnchorInfo anchorInfo) {
        baseAnchors.get().add(anchorInfo);
        try {
            LettuceRedisUtils.pipeline(redisAsyncCommands -> {
                ArrayList<RedisFuture<?>> redisFutures = new ArrayList<>();
                for(AnchorInfo info : baseAnchors.get()){
                    /**
                     * 1.构建userId维度的链路消息  数据结构list:{key,list}
                     * key:userId,listValue:[{timestamp,state,businessId},{timestamp,state,businessId}]
                     */
                    SimpleAnchorInfo simpleAnchorInfo = SimpleAnchorInfo.builder().businessId(info.getBusinessId()).state(info.getState()).timestamp(info.getTimestamp()).build();
                    for(String id : info.getIds()){
                        redisFutures.add(redisAsyncCommands.lpush(id.getBytes(), JSON.toJSONString(simpleAnchorInfo).getBytes()));
                        redisFutures.add(redisAsyncCommands.expire(id.getBytes(), (DateUtil.endOfDay(new Date()).getTime() - DateUtil.current()) / 1000));
                    }

                    /**
                     * 2.构建消息模板维度的链路消息，数据接口hash:{key,hash}
                     * key:businessId,hashValue:{state,stateCount}
                     */
                    redisFutures.add(redisAsyncCommands.hincrby(String.valueOf(info.getBusinessId()).getBytes(),
                            String.valueOf(info.getState()).getBytes(), info.getIds().size()));
                    redisFutures.add(redisAsyncCommands.expire(String.valueOf(info.getBusinessId()).getBytes(), DateUtil.offsetDay(new Date(), 30).getTime()));
                }
                return redisFutures;
            });
        }catch (Exception e){
            log.error("MsgSink#invoke error: {}", Throwables.getStackTraceAsString(e));
        } finally {
            lastClearTime.set(System.currentTimeMillis());
            baseAnchors.get().clear();
        }
    }

    /**
     * 离线数据存入hive
     * @param anchorInfo
     */
    private void offlineDate(AnchorInfo anchorInfo) {
    }
}
```
